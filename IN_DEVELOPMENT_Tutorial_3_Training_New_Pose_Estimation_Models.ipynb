{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pose Estimation - Training Models from New Datasets\n",
        "\n",
        "References: https://docs.ultralytics.com/"
      ],
      "metadata": {
        "id": "26NhmulM9Nx_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXM8fZY4839G"
      },
      "outputs": [],
      "source": [
        "#!pip install -U ultralytics\n",
        "#!pip install datasets\n",
        "import ultralytics\n",
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a New Model to Detect Hand Keypoints\n",
        "\n",
        "As we saw in the live demo, the out-of-the-box pose estimation model does not support articulation of finger movement or other fine hand gestures. We will now use the Hand Keypoints dataset (available through the Ultralytics API) to train a new model that is capable of this functionality, and demo it live on our webcam.\n",
        "\n",
        "https://docs.ultralytics.com/datasets/pose/hand-keypoints/"
      ],
      "metadata": {
        "id": "LPmsuJWP9TNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load copy of pretrained model and train on Hand Keypoints dataset:\n",
        "\n",
        "hand_model = YOLO(\"yolo11n-pose.pt\")  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Train the model\n",
        "hand_train_results = hand_model.train(data=\"hand-keypoints.yaml\", epochs=100, imgsz=640)"
      ],
      "metadata": {
        "id": "IQF_QFsk9CTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run live webcam inference to show how our model will now pick up the hand keypoints rather than the original body keypoints from earlier demo:\n",
        "\n",
        "#hand_model = YOLO(\"path/to/updates_model.pt\")\n",
        "results = hand_model(source=0, show=True, conf=0.3, save=True)\n",
        "\n",
        "# May need to run this as a separate .py file instead of in a notebook cell"
      ],
      "metadata": {
        "id": "3Drn1M6Q9Ek3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a New Model to Detect Animal Poses:\n",
        "We have seen out-of-the-box inference based on the COCO dataset, as well as the new functionality provided by the Hand Keypoints dataset. Now we will train one last model, this time to predict keypoints on tigers.\n",
        "\n",
        "Finally, we will see how this new model can be used to run real-time inference on a video of tigers from YouTube (sorry, we didn't have any tigers available live for this demo).\n",
        "\n",
        "https://docs.ultralytics.com/datasets/pose/tiger-pose/"
      ],
      "metadata": {
        "id": "9HBKyjRbCixn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a fresh copy of the pretrained model:\n",
        "tiger_model = YOLO(\"yolo11n-pose.pt\")  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Train the model on Tiger-Pose dataset:\n",
        "tiger_train_results = tiger_model.train(data=\"tiger-pose.yaml\", epochs=100, imgsz=640)"
      ],
      "metadata": {
        "id": "v7YyGoH-CMRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo real-time inference using video of tiger:\n",
        "\n",
        "# Load Tiger-Pose trained model:\n",
        "# tiger_model = YOLO(\"path/to/updated_model.pt\")\n",
        "\n",
        "# Run inference\n",
        "results = tiger_model.predict(source=\"https://youtu.be/MIBAT6BGE6U\", show=True)\n",
        "# May need to run this as a separate .py file instead of in a notebook cell"
      ],
      "metadata": {
        "id": "u2ae5MiYG4F8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}